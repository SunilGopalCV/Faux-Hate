{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1kpTtY06KmUvm591Ugw5cdxR_ChluK7hL","authorship_tag":"ABX9TyO2iKnsfPjDoOpaQQ5OLrIX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# XLM-RoBERTa Model for Binary Faux-Hate Detection"],"metadata":{"id":"R8IXQqukjjM_"}},{"cell_type":"markdown","source":["## Libraries"],"metadata":{"id":"w_QQthXWj4qC"}},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from transformers import XLMRobertaModel, XLMRobertaTokenizer\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"__Mlwp7Pj7QF","executionInfo":{"status":"ok","timestamp":1731733267814,"user_tz":-330,"elapsed":23507,"user":{"displayName":"sunil gopal","userId":"13101888373302829361"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## 1. Getting dataset and preparing tokenizer (xlm-roberta-base)"],"metadata":{"id":"tgvE0NdZn5-p"}},{"cell_type":"code","source":["data = pd.read_csv(\"/content/drive/MyDrive/Icon Conference/Data/Cleaned_Task_A.csv\")\n","tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n","\n","MAX_LEN = 128"],"metadata":{"id":"Q4cv79AaLfGN","executionInfo":{"status":"ok","timestamp":1731733272746,"user_tz":-330,"elapsed":4947,"user":{"displayName":"sunil gopal","userId":"13101888373302829361"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e5ae0899-3456-4286-e082-e86881f5afa7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["##2. Creating custom dataset and dataloaders"],"metadata":{"id":"rl7b1_EKoSyc"}},{"cell_type":"code","source":["class FauxHateDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_len):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        row = self.data.iloc[index]\n","        text = row['Tweet']\n","        label_faux = row['Fake']\n","        label_hate = row['Hate']\n","\n","        # Tokenize the text\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            max_length=self.max_len,\n","            truncation=True,\n","            padding='max_length',\n","            add_special_tokens=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels_faux': torch.tensor(label_faux, dtype=torch.long),\n","            'labels_hate': torch.tensor(label_hate, dtype=torch.long)\n","        }"],"metadata":{"id":"IdytDq7GLhSM","executionInfo":{"status":"ok","timestamp":1731733272747,"user_tz":-330,"elapsed":28,"user":{"displayName":"sunil gopal","userId":"13101888373302829361"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["data = data.dropna(subset=['Tweet'])\n","\n","# Split the data into training and validation sets\n","train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)  # 80% train, 20% validation\n","\n","# Create dataset and dataloaders\n","train_dataset = FauxHateDataset(train_data, tokenizer, MAX_LEN)\n","val_dataset = FauxHateDataset(val_data, tokenizer, MAX_LEN)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# Example to check one batch\n","batch = next(iter(train_dataloader))\n","print(batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w7KzAJreLwOb","executionInfo":{"status":"ok","timestamp":1731733272749,"user_tz":-330,"elapsed":24,"user":{"displayName":"sunil gopal","userId":"13101888373302829361"}},"outputId":"0053a3b8-f629-475e-9c9b-880aeb374c62"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[    0,   873, 18788,  ...,     1,     1,     1],\n","        [    0,  3036,   350,  ...,     1,     1,     1],\n","        [    0,  9925,    83,  ...,     1,     1,     1],\n","        ...,\n","        [    0,  1184,    13,  ...,     1,     1,     1],\n","        [    0,  7224, 73601,  ...,     1,     1,     1],\n","        [    0, 49191,  7525,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'labels_faux': tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n","        1, 1, 0, 1, 1, 1, 0, 0]), 'labels_hate': tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n","        1, 1, 1, 1, 1, 0, 0, 0])}\n"]}]},{"cell_type":"markdown","source":["## 3. Defining model architecture"],"metadata":{"id":"Jf8RWVzMoqQQ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import XLMRobertaModel, XLMRobertaTokenizer\n","\n","class FauxHateDetector(nn.Module):\n","    def __init__(self, model_name='xlm-roberta-base', num_labels_task1=2, num_labels_task2=2):\n","        super(FauxHateDetector, self).__init__()\n","        self.model = XLMRobertaModel.from_pretrained(model_name)\n","\n","        # Separate classification heads for 'faux' and 'hate'\n","        self.classifier_hate = nn.Linear(self.model.config.hidden_size, num_labels_task2)\n","        self.classifier_faux = nn.Linear(self.model.config.hidden_size, num_labels_task1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        # Get the embeddings from the base XLM-RoBERTa model\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n","\n","        # Task-specific heads\n","        faux_logits = self.classifier_faux(cls_output)\n","        hate_logits = self.classifier_hate(cls_output)\n","\n","        return faux_logits, hate_logits\n"],"metadata":{"id":"XgMNrrGkL32w","executionInfo":{"status":"ok","timestamp":1731733272750,"user_tz":-330,"elapsed":16,"user":{"displayName":"sunil gopal","userId":"13101888373302829361"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## 4.  Fitting the model to data (training)"],"metadata":{"id":"wYgAGOznpVih"}},{"cell_type":"code","source":["from transformers import AdamW\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import accuracy_score\n","\n","# Define training function\n","def train(model, dataloader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0\n","    for batch in dataloader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels_faux = batch['labels_faux'].to(device)\n","        labels_hate = batch['labels_hate'].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        faux_logits, hate_logits = model(input_ids, attention_mask)\n","\n","        # Calculate losses for both tasks\n","        loss_faux = criterion(faux_logits, labels_faux)\n","        loss_hate = criterion(hate_logits, labels_hate)\n","\n","        # Combined loss (weighted if needed)\n","        loss = loss_faux + loss_hate\n","        total_loss += loss.item()\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    return avg_loss\n"],"metadata":{"id":"cR7bM0-ML4-a","executionInfo":{"status":"ok","timestamp":1731733272750,"user_tz":-330,"elapsed":16,"user":{"displayName":"sunil gopal","userId":"13101888373302829361"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## 5. Making predictions and evaluating a model (inference"],"metadata":{"id":"RID1CahOpfo0"}},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","# Define evaluation function\n","def evaluate(model, dataloader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    all_labels_faux, all_preds_faux = [], []\n","    all_labels_hate, all_preds_hate = [], []\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels_faux = batch['labels_faux'].to(device)\n","            labels_hate = batch['labels_hate'].to(device)\n","\n","            # Forward pass\n","            faux_logits, hate_logits = model(input_ids, attention_mask)\n","\n","            # Calculate losses\n","            loss_faux = criterion(faux_logits, labels_faux)\n","            loss_hate = criterion(hate_logits, labels_hate)\n","            loss = loss_faux + loss_hate\n","            total_loss += loss.item()\n","\n","            # Store labels and predictions for accuracy metrics\n","            all_labels_faux.extend(labels_faux.cpu().numpy())\n","            all_preds_faux.extend(torch.argmax(faux_logits, dim=1).cpu().numpy())\n","\n","            all_labels_hate.extend(labels_hate.cpu().numpy())\n","            all_preds_hate.extend(torch.argmax(hate_logits, dim=1).cpu().numpy())\n","\n","    avg_loss = total_loss / len(dataloader)\n","    report_faux = classification_report(all_labels_faux, all_preds_faux)\n","    report_hate = classification_report(all_labels_hate, all_preds_hate)\n","\n","    return avg_loss, report_faux, report_hate\n"],"metadata":{"id":"8C9NapP-L7Gc","executionInfo":{"status":"ok","timestamp":1731733272750,"user_tz":-330,"elapsed":15,"user":{"displayName":"sunil gopal","userId":"13101888373302829361"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## 6. Training and Testing loop"],"metadata":{"id":"6VCBAH-fpoHT"}},{"cell_type":"code","source":["# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Model, criterion, optimizer\n","model = FauxHateDetector().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = AdamW(params = model.parameters(), lr=2e-5)\n","\n","# Variables to track the best model\n","best_val_loss = float('inf')\n","save_path = \"/content/drive/MyDrive/Icon Conference/multitaskXLMRoBERTa/best_model.pth\"\n","torch.manual_seed(42)\n","\n","# Training loop\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_dataloader, optimizer, criterion, device)\n","    val_loss, faux_report, hate_report = evaluate(model, val_dataloader, criterion, device)\n","\n","    # Check if validation loss improved and save the best model\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save(model.state_dict(), save_path)\n","        print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n","\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    print(f\"Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}\")\n","    print(\"Faux Detection Report:\\n\", faux_report)\n","    print(\"Hate Detection Report:\\n\", hate_report)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5prUXtBML-Qi","outputId":"96702efd-351d-41a6-fc29-3c9aa187258c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## 7. Model evaluation and visualization"],"metadata":{"id":"fsOHQTtYpt6L"}},{"cell_type":"code","source":["import torch\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","# Load the best model\n","best_model = FauxHateDetector().to(device)\n","best_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Icon Conference/multitaskXLMRoBERTa/best_model.pth\"))\n","best_model.eval()\n","\n","# Evaluate and visualize on the test set\n","def evaluate_and_visualize_test(model, dataloader, criterion, device):\n","    model.eval()\n","    all_labels_faux, all_preds_faux = [], []\n","    all_labels_hate, all_preds_hate = [], []\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels_faux = batch['labels_faux'].to(device)\n","            labels_hate = batch['labels_hate'].to(device)\n","\n","            # Forward pass\n","            faux_logits, hate_logits = model(input_ids, attention_mask)\n","\n","            # Calculate loss\n","            loss_faux = criterion(faux_logits, labels_faux)\n","            loss_hate = criterion(hate_logits, labels_hate)\n","            total_loss += (loss_faux + loss_hate).item()\n","\n","            # Store predictions and labels\n","            all_labels_faux.extend(labels_faux.cpu().numpy())\n","            all_preds_faux.extend(torch.argmax(faux_logits, dim=1).cpu().numpy())\n","            all_labels_hate.extend(labels_hate.cpu().numpy())\n","            all_preds_hate.extend(torch.argmax(hate_logits, dim=1).cpu().numpy())\n","\n","    avg_loss = total_loss / len(dataloader)\n","    print(f\"Test Loss: {avg_loss:.4f}\")\n","\n","    # Print classification reports\n","    print(\"Faux Detection Report:\\n\", classification_report(all_labels_faux, all_preds_faux))\n","    print(\"Hate Detection Report:\\n\", classification_report(all_labels_hate, all_preds_hate))\n","\n","    # Confusion matrices\n","    conf_matrix_faux = confusion_matrix(all_labels_faux, all_preds_faux)\n","    conf_matrix_hate = confusion_matrix(all_labels_hate, all_preds_hate)\n","\n","    # Plot confusion matrices\n","    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n","\n","    sns.heatmap(conf_matrix_faux, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n","    axes[0].set_title(\"Confusion Matrix for Faux Detection\")\n","    axes[0].set_xlabel(\"Predicted Labels\")\n","    axes[0].set_ylabel(\"True Labels\")\n","\n","    sns.heatmap(conf_matrix_hate, annot=True, fmt='d', cmap='Reds', ax=axes[1])\n","    axes[1].set_title(\"Confusion Matrix for Hate Detection\")\n","    axes[1].set_xlabel(\"Predicted Labels\")\n","    axes[1].set_ylabel(\"True Labels\")\n","\n","    plt.show()\n","\n","# Assuming `test_dataloader` is already created for test data\n","evaluate_and_visualize_test(best_model, val_dataloader, criterion, device)\n"],"metadata":{"id":"efoBDkGpj9qW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9SqzPyXxp7qY"},"execution_count":null,"outputs":[]}]}
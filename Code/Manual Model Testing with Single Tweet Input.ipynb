{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1rRI7NpuRIkl8fSK9euaObc4HabFwEUjF","authorship_tag":"ABX9TyNzDt6N1GAoNN1bHW8wE3e1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Manual Model Testing with Single Tweet Input"],"metadata":{"id":"tCxaKuEYqhXj"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import XLMRobertaModel, XLMRobertaTokenizer"],"metadata":{"id":"ElRRvpwMl1T5","executionInfo":{"status":"ok","timestamp":1731603319624,"user_tz":-330,"elapsed":637,"user":{"displayName":"sunil gopal","userId":"13101888373302829361"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class FauxHateDetector(nn.Module):\n","    def __init__(self, model_name='xlm-roberta-base', num_labels_task1=2, num_labels_task2=2):\n","        super(FauxHateDetector, self).__init__()\n","        self.model = XLMRobertaModel.from_pretrained(model_name)\n","\n","        # Separate classification heads for 'faux' and 'hate'\n","        self.classifier_faux = nn.Linear(self.model.config.hidden_size, num_labels_task1)\n","        self.classifier_hate = nn.Linear(self.model.config.hidden_size, num_labels_task2)\n","\n","    def forward(self, input_ids, attention_mask):\n","        # Get the embeddings from the base XLM-RoBERTa model\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n","\n","        # Task-specific heads\n","        faux_logits = self.classifier_faux(cls_output)\n","        hate_logits = self.classifier_hate(cls_output)\n","\n","        return faux_logits, hate_logits"],"metadata":{"id":"osm5e3wSltr0","executionInfo":{"status":"ok","timestamp":1731603319625,"user_tz":-330,"elapsed":5,"user":{"displayName":"sunil gopal","userId":"13101888373302829361"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfxlEeNek8FA","executionInfo":{"status":"ok","timestamp":1731603334097,"user_tz":-330,"elapsed":14476,"user":{"displayName":"sunil gopal","userId":"13101888373302829361"}},"outputId":"473e0299-f167-45a9-b71a-dbcb0b5e8e0e"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-d16a5376b465>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(\"/content/drive/MyDrive/Icon Conference/multitaskXLMRoBERTa/best_model.pth\", map_location=torch.device(device)))\n"]}],"source":["import torch\n","from transformers import XLMRobertaTokenizer\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the tokenizer and the best model\n","tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n","model = FauxHateDetector().to(device)\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/Icon Conference/multitaskXLMRoBERTa/best_model.pth\", map_location=torch.device(device)))\n","model.eval()\n","\n","def predict_single_tweet(model, tokenizer, tweet, max_len=128):\n","    # Tokenize the tweet\n","    encoding = tokenizer.encode_plus(\n","        tweet,\n","        max_length=max_len,\n","        truncation=True,\n","        padding='max_length',\n","        add_special_tokens=True,\n","        return_tensors='pt'\n","    )\n","\n","    # Move input to the device\n","    input_ids = encoding['input_ids'].to(device)\n","    attention_mask = encoding['attention_mask'].to(device)\n","\n","    # Get predictions from the model\n","    with torch.no_grad():\n","        faux_logits, hate_logits = model(input_ids, attention_mask)\n","\n","    # Get the predicted class (0 or 1) for both faux and hate tasks\n","    faux_prediction = torch.argmax(faux_logits, dim=1).item()\n","    hate_prediction = torch.argmax(hate_logits, dim=1).item()\n","\n","    # Convert predictions to readable labels\n","    faux_label = \"Fake\" if faux_prediction == 1 else \"Not Fake\"\n","    hate_label = \"Hate\" if hate_prediction == 1 else \"Not Hate\"\n","\n","    return faux_label, hate_label\n"]},{"cell_type":"code","source":["import re\n","def clean_text(text):\n","    if not isinstance(text, str):\n","        return ''\n","    text = str(text)\n","    text = text.lower()\n","    text = re.sub(r'<br>', ' ', text)\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n","    text = re.sub(r'@\\w+|#+', '', text)\n","    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n","    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","\n","    return text.strip()"],"metadata":{"id":"HaEYR3ZymuMT","executionInfo":{"status":"ok","timestamp":1731603507149,"user_tz":-330,"elapsed":487,"user":{"displayName":"sunil gopal","userId":"13101888373302829361"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Example Usage of Model"],"metadata":{"id":"FxfO5adVqvqW"}},{"cell_type":"code","source":["tweet = \"@SwetaSinghAT Madam ap 2000 ke note me nano GPS chip dhundo,chai me cheeni koi aur dhund lega.yad h na yh iconic episode jab ap g huzoori me fake news chala rhi thi.Waise Galwan me hamare jawan shaheed hue na kyu hue us par kabhi minister ya PM se sawal k\"\n","tweet = clean_text(tweet)\n","faux_label, hate_label = predict_single_tweet(model, tokenizer, tweet)\n","print(f\"Faux Prediction: {faux_label}\")\n","print(f\"Hate Prediction: {hate_label}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r7JhbRMUmaMH","executionInfo":{"status":"ok","timestamp":1731604582819,"user_tz":-330,"elapsed":486,"user":{"displayName":"sunil gopal","userId":"13101888373302829361"}},"outputId":"11677930-e8c2-43d1-d114-36478eabc260"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Faux Prediction: Not Fake\n","Hate Prediction: Not Hate\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"uAmeOWAIrFCy"},"execution_count":null,"outputs":[]}]}